"""`damask_parse.readers.py`"""

from pathlib import Path

import pandas
import re
import numpy as np

from damask_parse.utils import get_header

__all__ = [
    'read_table',
    'read_geom',
]


def read_table(path, use_dataframe=False, combine_array_columns=True,
               ignore_duplicate_cols=False, check_header=True):
    """Read the data from a DAMASK-generated ASCII table file, as generated by
    the DAMASK post-processing command named `postResults`.

    Parameters
    ----------
    path : str or Path
        Path to the DAMASK table file.
    use_dataframe : bool, optional
        If True, a Pandas DataFrame is returned. Otherwise, a dict of Numpy
        arrays is returned. By default, set to False.        
    combine_array_columns : bool, optional
        If True, columns that represent elements of an array (e.g. stress) are
        combined into a single column. By default, set to True.
    ignore_duplicate_cols : bool, optional
        If True, duplicate columns (as detected by the `mangle_dupe_cols`
        option of `pandas_read_csv` function) are ignored. Otherwise, an
        exception is raised. By default, set to False.
    check_header : bool, optional
        Check that the command `postResults` appears in the header, i.e. check
        that the file is indeed likely to be a DAMASK table file. By default,
        set to True.

    Returns
    -------
    outputs : dict
        The data in the table file, represented either as a Pandas DataFrame
        (if `use_dataframe` is True) or a dict of Numpy arrays (if
        `use_dataframe` if False).

    TODO: parse all "array-like" columns in one-dimensional Numpy arrays, and
    then reshape known columns into correct shapes.

    """

    arr_shape_lookup = {
        12: [4, 3],
        9: [3, 3],
        3: [3],
        4: [4],
    }

    header = get_header(path)
    header_num = len(header)

    if check_header:
        if 'postResults' not in header[0]:
            msg = (
                '"postResults" does not appear in the header of the supposed '
                'table file. If you want to ignore this fact, call the '
                '`read_table` function with the parameter '
                '`check_header=False`.'
            )
            raise ValueError(msg)

    df = pandas.read_csv(str(path), delim_whitespace=True, header=header_num)

    if not ignore_duplicate_cols:
        if np.any(df.columns.str.replace(r'(\.\d+)$', '').duplicated()):
            msg = (
                'It appears there are duplicated columns in the table. If you '
                'want to ignore this fact, call the `read_table` function with'
                ' the parameter `ignore_duplicate_cols=True`.'
            )
            raise ValueError(msg)

    arr_sizes = None
    if combine_array_columns or not use_dataframe:
        # Find number of elements for each "array" column:
        arr_sizes = {}
        for header in df.columns.values:
            match = re.match(r'([0-9]+)_(.+)', header)
            if match:
                arr_name = match.group(2)
                if arr_name in arr_sizes:
                    arr_sizes[arr_name] += 1
                else:
                    arr_sizes.update({
                        arr_name: 1
                    })

        # Check for as yet "unsupported" array dimensions:
        bad_num_elems = set(arr_sizes.values()) - set(arr_shape_lookup.keys())
        if len(bad_num_elems) > 0:
            msg = (
                '"Array" columns must have one of the following number of '
                'elements: {}. However, there are columns with the following '
                'numbers of elements: {}'.format(
                    list(arr_shape_lookup.keys()), list(bad_num_elems)
                )
            )
            raise ValueError(msg)

    if combine_array_columns:
        # Add arrays as single columns:
        for arr_name, arr_size in arr_sizes.items():
            arr_idx = ['{}_{}'.format(i, arr_name)
                       for i in range(1, arr_size + 1)]
            df[arr_name] = df[arr_idx].values.tolist()
            # Remove individual array columns:
            df = df.drop(arr_idx, axis=1)

    outputs = df

    if not use_dataframe:
        # Transform each column into a Numpy array:
        arrays = {}
        for header in df.columns.values:
            val = np.array(df[header])

            if header in arr_sizes:
                shp = tuple([-1] + arr_shape_lookup[arr_sizes[header]])
                val = np.array([*df[header]]).reshape(shp)

            arrays.update({
                header: val
            })
            outputs = arrays

    return outputs


def read_geom(geom_path):
    """Parse a DAMASK geometry file.

    Parameters
    ----------
    geom_path : str or Path
        Path to the DAMASK geometry file.

    Returns
    -------
    volume_element : dict
        Dictionary that represents the volume element parsed from the geometry
        file, with keys:
            grain_idx : ndarray of dimension three
                A mapping that determines the grain index for each voxel.
            orientations: ndarray of shape (N, 3)
                Orientation of each grain in terms of three Euler angles.

    Notes
    -----
    Expected format of the geometry file is, by line numbers:
        1. The number of lines in the "header"
        2. The Command invoked to generate this file and the version of that
           command
        3. Grid discretization resolution
        4. RVE size
        5. RVE origin
        6. Homogenisation scheme?
        7. Number of microstructures (i.e. grains?)
        8. <microstructure> part, which specifies the crystallite and phase
           indices from which each grain is constructed.
        .. <texture> part, which specifies, for each grain, the orientation.
    """

    with geom_path.open() as handle:

        micro_str = '<microstructure>'
        texture_str = '<texture>'

        grid_size = None
        grain_idx_2d = None
        micro_lns = []
        texture_lns = []

        num_header_lns = 0
        parse = ''

        for ln_idx, ln in enumerate(handle.readlines()):

            ln_s = ln.strip()
            ln_split = ln_s.split()

            if ln_idx == 0:
                num_header_lns = int(ln_split[0])

            if ln_idx == 2:
                grid_size = [int(ln_split[i]) for i in (2, 4, 6)]
                grain_idx_2d = np.zeros((grid_size[1] * grid_size[2],
                                         grid_size[0]), dtype=int)
            if ln_idx > num_header_lns:
                parse = 'grain_idx'
            elif ln_s == micro_str:
                parse = 'micro'
                continue
            elif ln_s == texture_str:
                parse = 'texture'
                # print('continuing...')
                continue

            if parse == 'micro':
                micro_lns.append(ln_s)
            elif parse == 'texture':
                texture_lns.append(ln_s)
            elif parse == 'grain_idx':
                arr_idx = ln_idx - (num_header_lns + 1)
                grain_idx_2d[arr_idx] = [int(i) for i in ln_split]

    # Parse texture lines

    # Don't need the section name lines e.g. "[Grain03]":
    texture_lns = [i for idx, i in enumerate(texture_lns) if idx % 2 != 0]

    # Assume using Euler angles (i.e. lines will start with "(guass)"):
    orientations = np.array([
        [float(j) for idx, j in enumerate(i.split()) if idx in [2, 4, 6]]
        for i in texture_lns
    ])

    grain_idx = grain_idx_2d.reshape(grid_size[::-1]).swapaxes(0, 2)

    volume_element = {
        'grain_idx': grain_idx,
        'orientations': orientations,
    }

    return volume_element
